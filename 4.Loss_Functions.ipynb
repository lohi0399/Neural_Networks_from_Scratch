{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function --> Key to optimize our network\n",
    "\n",
    "# 1. Categorical Cross Entropy ---> Generally used for classification\n",
    "\n",
    "# Say I have the the following scenario where in the output values are [Red, Green , Blue], and we have the following batch of data:\n",
    "import numpy as np \n",
    "\n",
    "outputs = [[0.7,0.1,0.2],\n",
    "           [0.1,0.5,0.4],\n",
    "           [0.02,0.9,0.08]]\n",
    "\n",
    "true_labels = [[1,0,0],\n",
    "               [0,1,0],\n",
    "               [0,1,0]]\n",
    "\n",
    "class CCE:\n",
    "    def ce_loss(self,outputs,true_labels):\n",
    "        l = - (true_labels * np.log(outputs))\n",
    "        self.loss = np.sum(l,axis=1,keepdims=True)\n",
    "\n",
    "ce = CCE()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35667494],\n",
       "       [0.69314718],\n",
       "       [0.10536052]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce.ce_loss(outputs,true_labels)\n",
    "ce.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce.loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " NOTE: The base of the log is generally taken as 'e'. Even if we take another base it doesn't really matter since any base can be represented in terms of 'e' and base 2 basically turn out be the scaling factor. I only adavantage of using base 2 is that it is faster to comppute, but anyway CE is never the cotly part of NN for it not something to be overly concerend with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "# Fancy indexing in NumPy\n",
    "\n",
    "softmax_outputs = np.array([\n",
    "    [0.7, 0.1, 0.2],  # Row 0\n",
    "    [0.1, 0.5, 0.4],  # Row 1\n",
    "    [0.02, 0.9, 0.08] # Row 2\n",
    "])\n",
    "\n",
    "class_targets = [0, 1, 1]  # Target indices\n",
    "\n",
    "print(softmax_outputs[[0,1,2],class_targets]) # [rows,columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35667494, 0.69314718, 0.10536052])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -np.log( softmax_outputs[[0,1,2],class_targets])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38506088005216804"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average loss\n",
    "\n",
    "np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
